{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "989ab5d7-156f-4de9-b9eb-dffd8aee213a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apache Spark and PySpark are closely related but serve different purposes within the realm of big data processing. \n",
    "\n",
    "Here's a comparison to help understand their differences and use cases:\n",
    "\n",
    "### Apache Spark\n",
    "- **Definition**: Apache Spark is an open-source distributed computing system designed for speed and ease of use in processing large-scale data.\n",
    "- **Languages**: Spark's core API is written in Scala, but it also provides APIs for Java, Python, and R.\n",
    "- **Components**: Spark consists of several key components:\n",
    "  - **Spark Core**: The underlying engine responsible for scheduling, distributing, and monitoring applications.\n",
    "  - **Spark SQL**: Module for structured data processing, allowing SQL queries.\n",
    "  - **Spark Streaming**: Enables processing of real-time data streams.\n",
    "  - **MLlib**: Library for machine learning.\n",
    "  - **GraphX**: API for graphs and graph-parallel computation.\n",
    "\n",
    "### PySpark\n",
    "- **Definition**: PySpark is the Python API for Apache Spark, allowing users to interact with Spark through Python.\n",
    "- **Usage**: PySpark is used for integrating Python with Spark’s capabilities, making it accessible for those who are familiar with Python programming.\n",
    "- **Components**: PySpark exposes the functionalities of Spark Core, Spark SQL, Spark Streaming, MLlib, and GraphX through Python.\n",
    "\n",
    "### Key Differences\n",
    "- **Language**: Spark provides APIs in multiple languages (Scala, Java, Python, R), while PySpark specifically caters to Python.\n",
    "\n",
    "### Use Cases\n",
    "- **Apache Spark**: Suitable for applications where you need the flexibility of choosing among multiple languages or need to leverage the full power of the Spark ecosystem for large-scale distributed computing.\n",
    "\n",
    "In summary, PySpark is essentially a Python wrapper for Apache Spark, enabling Python developers to leverage Spark's distributed computing capabilities. Apache Spark provides the core functionality and infrastructure, while PySpark provides a user-friendly interface for Python users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4b93ae-5673-4c46-994c-9bbb8ca36cce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**RDD (Resilient Distributed Dataset)** is a fundamental data structure of Apache Spark. \n",
    "\n",
    "- **Definition**: An RDD is a fault-tolerant collection of elements that can be operated on in parallel across a distributed cluster.\n",
    "- **Properties**:\n",
    "  - **Immutable**: Once created, RDDs cannot be changed.\n",
    "  - **Distributed**: Data is distributed across multiple nodes in a cluster.\n",
    "  - **Fault-Tolerant**: If a node fails, data can be recomputed using lineage information (the sequence of transformations that created the RDD).\n",
    "- **Creation**: RDDs can be created from:\n",
    "  - **Data in storage**: Such as HDFS, S3, or local file systems.\n",
    "  - **Parallelizing**: An existing collection in the driver program (e.g., a list or set).\n",
    "- **Operations**:\n",
    "  - **Transformations**: Lazy operations that define a new RDD (e.g., `map`, `filter`). These are not executed immediately but are recorded in the RDD's lineage.\n",
    "  - **Actions**: Operations that trigger computation and return a result (e.g., `collect`, `count`). They execute the transformations to produce an output.\n",
    "\n",
    "RDDs enable distributed processing and are the backbone of Spark’s high performance and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6edcd00-7e8b-4e20-8a8d-bb1139b427fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**RDD (Resilient Distributed Dataset)** is a fundamental data structure of Apache Spark. Here’s a concise summary:\n",
    "\n",
    "- **Definition**: An RDD is a fault-tolerant collection of elements that can be operated on in parallel across a distributed cluster.\n",
    "- **Properties**:\n",
    "  - **Immutable**: Once created, RDDs cannot be changed.\n",
    "  - **Distributed**: Data is distributed across multiple nodes in a cluster.\n",
    "  - **Fault-Tolerant**: If a node fails, data can be recomputed using lineage information (the sequence of transformations that created the RDD).\n",
    "- **Creation**: RDDs can be created from:\n",
    "  - **Data in storage**: Such as HDFS, S3, or local file systems.\n",
    "  - **Parallelizing**: An existing collection in the driver program (e.g., a list or set).\n",
    "- **Operations**:\n",
    "  - **Transformations**: Lazy operations that define a new RDD (e.g., `map`, `filter`). These are not executed immediately but are recorded in the RDD's lineage.\n",
    "  - **Actions**: Operations that trigger computation and return a result (e.g., `collect`, `count`). They execute the transformations to produce an output.\n",
    "\n",
    "RDDs enable distributed processing and are the backbone of Spark’s high performance and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84521e2-51a2-47ed-b23a-626a331e8307",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 40, 60, 80, 100, 120, 140]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a Python list\n",
    "data = [1, 2, 3, 4, 5, 6, 7]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform a simple transformation (e.g., multiply each element by 20)\n",
    "transformed_rdd = rdd.map(lambda x: x * 20)\n",
    "\n",
    "# Collect the results\n",
    "result = transformed_rdd.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372a3ba1-20d8-477a-adeb-7e0cdea44ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Filter out even numbers\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Perform a reduction to sum the numbers\n",
    "sum_even = even_rdd.reduce(lambda a, b: a + b)\n",
    "print(sum_even) #2 + 4 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03e59b3-4a6a-4dbd-a2b9-c14b44915740",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Text File\n",
    "\n",
    "In Gotham's night, a shadow flies,\n",
    "With cape unfurled, beneath dark skies.\n",
    "A guardian fierce, with silent might,\n",
    "He guards the weak throughout the night.\n",
    "\n",
    "With heart of gold and soul of steel,\n",
    "He fights for justice, true and real.\n",
    "The Batman soars, a darkened knight,\n",
    "Our city's hope, our guiding light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1857f8-c536-4f4d-be86-91b27d140849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shadow: 1\nflies,: 1\nunfurled,: 1\nbeneath: 1\ndark: 1\nfierce,: 1\nmight,: 1\nnight.: 1\n: 1\nheart: 1\nof: 2\ngold: 1\nsteel,: 1\ntrue: 1\nThe: 1\ndarkened: 1\nknight,: 1\ncity's: 1\nIn: 1\nGotham's: 1\nnight,: 1\na: 2\nWith: 2\ncape: 1\nskies.: 1\nA: 1\nguardian: 1\nwith: 1\nsilent: 1\nHe: 2\nguards: 1\nthe: 2\nweak: 1\nthroughout: 1\nand: 2\nsoul: 1\nfights: 1\nfor: 1\njustice,: 1\nreal.: 1\nBatman: 1\nsoars,: 1\nOur: 1\nhope,: 1\nour: 1\nguiding: 1\nlight.: 1\n"
     ]
    }
   ],
   "source": [
    "# Read a text file into an RDD\n",
    "file_rdd = sc.textFile(\"dbfs:/FileStore/poem.txt\")\n",
    "\n",
    "# Split lines into words\n",
    "words_rdd = file_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Map each word to a tuple (word, 1)\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key (word) to count occurrences\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and print the results\n",
    "word_counts = word_counts_rdd.collect()\n",
    "for word, count in word_counts:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ca98e8-a50d-46e7-a2ce-a65048ba3b49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+--------+\n| Name|Age|       City|Category|\n+-----+---+-----------+--------+\n|krish| 33|     Rajkot|  Gifted|\n|bruce| 41|     Rajkot|  Gifted|\n| king| 19|Gandhinagar|  Gifted|\n|steve| 25|     Rajkot|    null|\n| ravi| 28|Gandhinagar|    null|\n+-----+---+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples\n",
    "data = [(\"krish\", 33, \"Rajkot\", \"Gifted\"), (\"bruce\", 41, \"Rajkot\", \"Gifted\"), (\"king\", 19, \"Gandhinagar\", \"Gifted\"), (\"steve\", 25, \"Rajkot\",None), (\"ravi\", 28, \"Gandhinagar\",None)]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"Name\", \"Age\", 'City',\"Category\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Read a file into a DataFrame\n",
    "# df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "# df = spark.read.json(\"data.json\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8071a71-18e9-46de-a942-22bce6da5123",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Performing DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39322c33-8255-4df2-961a-09f816db9da9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| Name|age|\n+-----+---+\n|krish| 33|\n|bruce| 41|\n| king| 19|\n|steve| 25|\n| ravi| 28|\n+-----+---+\n\n+-----+---+------+--------+\n| Name|Age|  City|Category|\n+-----+---+------+--------+\n|krish| 33|Rajkot|  Gifted|\n|bruce| 41|Rajkot|  Gifted|\n+-----+---+------+--------+\n\n+-----------+-----+\n|       City|count|\n+-----------+-----+\n|     Rajkot|    3|\n|Gandhinagar|    2|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "df.select(\"Name\", \"age\").show()\n",
    "\n",
    "# Filter rows based on a condition\n",
    "df.filter(df.Age > 30).show()\n",
    "\n",
    "# Group by a column and perform aggregation\n",
    "df.groupBy(\"City\").count().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "What is PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
